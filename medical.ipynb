{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7697892e-8a08-4a39-846c-d3c3f98febbf",
   "metadata": {},
   "source": [
    "# 加载 Qwen-1_8B-Chat-Int4 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f885b6-7eef-4a18-a53e-6afa899c1028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanyecyh/miniconda3/envs/QwenSFT/lib/python3.8/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/hanyecyh/miniconda3/envs/QwenSFT/lib/python3.8/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /home/hanyecyh/.cache/modelscope/hub/models/qwen/Qwen-1_8B-Chat-Int4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 11:46:25,201 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "/home/hanyecyh/miniconda3/envs/QwenSFT/lib/python3.8/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/home/hanyecyh/miniconda3/envs/QwenSFT/lib/python3.8/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/home/hanyecyh/miniconda3/envs/QwenSFT/lib/python3.8/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Some weights of the model checkpoint at /home/hanyecyh/.cache/modelscope/hub/models/qwen/Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.11.mlp.w2.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.17.mlp.w2.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 没下载模型就执行下面这句，因为huggingface没办法直接连，科学上网也没用，所以换成魔塔的了\n",
    "model_dir = snapshot_download('qwen/Qwen-1_8B-Chat-Int4')\n",
    "# 下载好模型的可以执行下面这句，直接拿到本地的模型目录\n",
    "model_dir = \"/home/hanyecyh/.cache/modelscope/hub/models/qwen/Qwen-1_8B-Chat-Int4\"\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f100b00-d76f-4a32-b2d2-f5f1bb4ed0eb",
   "metadata": {},
   "source": [
    "# 提示词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52121fa5-c955-4007-8eae-c802f776a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "由于没有提供具体的问题和对应的解决方案，我将以一个常见的过敏性鼻炎为例来回答这个问题。假设问题如下：“早上起来一直有过敏性鼻炎发作，怎么办？”在这种情况下，我们可以给出以下的JSON字符串：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"①\": \"避免接触可能引起过敏反应的物质\",\n",
      "  \"②\": \"服用抗过敏药物\"\n",
      "}\n",
      "```\n",
      "\n",
      "此JSON字符串中，“①”表示对过敏症状的具体治疗措施，比如“避免接触可能引起过敏反应的物质”。而“②”则表示使用抗过敏药物作为辅助治疗方式。\n",
      "\n",
      "如果你能提供更具体的场景或问题，我会很乐意为你进一步提供相应的JSON字符串。\n"
     ]
    }
   ],
   "source": [
    "Q='早上起来一直有过敏性鼻炎发作，怎么办？'\n",
    "\n",
    "prompt_template='''\n",
    "给定一句话：“%s”，请你按步骤要求工作。\n",
    "\n",
    "步骤1：识别用户问题中遇到身体上的常见症状，如感冒、发烧等。\n",
    "步骤2：根据问题，生成JSON格式字符串，格式为{\"①\":应对症状的一种做法, \"②\":针对症状的另一种做法}，根据具体症状给出一种或多种解决方案，分点作答\n",
    "\n",
    "请问，这个JSON字符串是：\n",
    "\n",
    "'''\n",
    "prompt = prompt_template%(Q,)\n",
    "# print(prompt)  # 给模型的命令\n",
    "\n",
    "resp, hist = model.chat(tokenizer, prompt, history=None)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b223658-87fd-4ad8-8fa3-ceccf2aa4837",
   "metadata": {},
   "source": [
    "# 批量整理训练所需数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f752a0-10b1-4290-b534-0e9f5f4581ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "flag0 = True\n",
    "flag1 = True\n",
    "flag2 = False\n",
    "Q = \"question\"\n",
    "A = \"\"\n",
    "i = 0\n",
    "\n",
    "with open('medical.txt','r',encoding='utf-8') as fp:\n",
    "    for line in fp:\n",
    "        if 'QUESTION' in line:\n",
    "            flag1 = True\n",
    "            flag2 = False\n",
    "            # 第一次的话前面没有样本，直接跳过\n",
    "            if i == 0:\n",
    "                continue\n",
    "            # 添加一个example(本次QUESTION不是第一次，上面还有一个example)\n",
    "            example={\n",
    "                'id': 'identity_{}'.format(i),\n",
    "                'conversations':[\n",
    "                    {\n",
    "                        'from':'user',\n",
    "                        'value':prompt_template%(Q,),\n",
    "                    },\n",
    "                    {\n",
    "                        'from':'assistant',\n",
    "                        'value':A,\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "            train_data.append(example)\n",
    "            # A不能无限累加，要重新置0\n",
    "            A = \"\"\n",
    "        elif 'ANSWER' in line:\n",
    "            flag1 = False\n",
    "            flag2 = True\n",
    "        elif flag1:\n",
    "            Q = line\n",
    "        else:\n",
    "            A += line\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    # 最后退出时，由于后面没QUESTION可以后处理，因此在此处还需要处理最后一批数据\n",
    "    example={\n",
    "        'id': 'identity_{}'.format(i),\n",
    "        'conversations':[\n",
    "            {\n",
    "                'from':'user',\n",
    "                'value':prompt_template%(Q,),\n",
    "            },\n",
    "            {\n",
    "                'from':'assistant',\n",
    "                'value':A,\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "    train_data.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1fa2d-d1b5-481f-8cbc-80dcd812838c",
   "metadata": {},
   "source": [
    "# 导出训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90f08d28-14e2-4fee-9e44-527f40ba4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('medical_train.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write(json.dumps(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22695eb-d75f-4c9f-ba79-ae5ed3df1ad4",
   "metadata": {},
   "source": [
    "# 微调模型，生成到output_qwen\n",
    "bash 模型脚本   模型路径   训练文件  \n",
    "bash finetune/finetune_qlora_single_gpu.sh -m /home/hanyecyh/.cache/modelscope/hub/models/qwen/Qwen-1_8B-Chat-Int4 -d /home/hanyecyh/python_code/Qwen/medical_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167537e3-0494-416e-8c94-d7227ae4aa43",
   "metadata": {},
   "source": [
    "# 加载SFT后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c365e29e-64fb-44d8-889b-1c317e2cd962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Some weights of the model checkpoint at /home/hanyecyh/python_code/Qwen/model1_8 were not used when initializing QWenLMHeadModel: ['transformer.h.11.mlp.w2.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.17.mlp.w2.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 151851. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    'output_qwen', # path to the output directory\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9f66a-83ab-45fe-9e88-0928c67975ea",
   "metadata": {},
   "source": [
    "# 利用微调后的模型，重新回答若干问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ff65b7-4830-4214-ac4e-80d8343d8bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "给定一句话：“不小心怀孕了要怎么避孕？”，请你按步骤要求工作。\n",
      "\n",
      "步骤1：识别用户问题中遇到身体上的常见症状，如感冒、发烧等。\n",
      "步骤2：根据问题，生成JSON格式字符串，格式为{\"①\":应对症状的一种做法, \"②\":针对症状的另一种做法}，根据具体症状给出一种或多种解决方案，分点作答\n",
      "\n",
      "请问，这个JSON字符串是：\n",
      "\n",
      "\n",
      "Q:不小心怀孕了要怎么避孕？\n",
      "A:症状:感冒\n",
      "应对症状:休息并多喝水\n",
      "应对症状:服用解热药\n",
      "应对症状:就医检查\n",
      "症状:发烧\n",
      "应对症状:多喝水\n",
      "应对症状:服用退烧药\n",
      "症状:恶心\n",
      "应对症状:休息并多喝水\n",
      "应对症状:就医检查\n",
      "症状:呕吐\n",
      "应对症状:休息并多喝水\n",
      "应对症状:就医检查\n",
      "\n",
      "\n",
      "给定一句话：“我早上起来有过敏性鼻炎怎么办”，请你按步骤要求工作。\n",
      "\n",
      "步骤1：识别用户问题中遇到身体上的常见症状，如感冒、发烧等。\n",
      "步骤2：根据问题，生成JSON格式字符串，格式为{\"①\":应对症状的一种做法, \"②\":针对症状的另一种做法}，根据具体症状给出一种或多种解决方案，分点作答\n",
      "\n",
      "请问，这个JSON字符串是：\n",
      "\n",
      "\n",
      "Q:我早上起来有过敏性鼻炎怎么办\n",
      "A:症状:过敏性鼻炎\n",
      "应对症状:避免接触过敏源\n",
      "针对症状:服用抗过敏药\n",
      "解决方案:\n",
      "1. 保持室内空气清新，避免尘螨和花粉等过敏原。\n",
      "2. 饮食上应避免食用可能引发过敏的食物，如海鲜、牛奶等。\n",
      "3. 使用空气净化器可以有效减少室内过敏原。\n",
      "4. 如果症状严重，可考虑就医治疗。\n",
      "\n",
      "\n",
      "给定一句话：“最近嘴唇和手脚一直干裂”，请你按步骤要求工作。\n",
      "\n",
      "步骤1：识别用户问题中遇到身体上的常见症状，如感冒、发烧等。\n",
      "步骤2：根据问题，生成JSON格式字符串，格式为{\"①\":应对症状的一种做法, \"②\":针对症状的另一种做法}，根据具体症状给出一种或多种解决方案，分点作答\n",
      "\n",
      "请问，这个JSON字符串是：\n",
      "\n",
      "\n",
      "Q:最近嘴唇和手脚一直干裂\n",
      "A:症状:嘴唇和手脚干裂\n",
      "应对症状:保持充足的水分摄入\n",
      "针对症状:使用润唇膏或者保湿霜\n",
      "解决方案:\n",
      "1.多喝水，保持身体水分充足\n",
      "2.避免口渴时过度饮水\n",
      "3.涂抹润唇膏或者保湿霜，保持嘴唇和手脚湿润\n",
      "\n",
      "\n",
      "给定一句话：“最近口腔感觉不卫生”，请你按步骤要求工作。\n",
      "\n",
      "步骤1：识别用户问题中遇到身体上的常见症状，如感冒、发烧等。\n",
      "步骤2：根据问题，生成JSON格式字符串，格式为{\"①\":应对症状的一种做法, \"②\":针对症状的另一种做法}，根据具体症状给出一种或多种解决方案，分点作答\n",
      "\n",
      "请问，这个JSON字符串是：\n",
      "\n",
      "\n",
      "Q:最近口腔感觉不卫生\n",
      "A:症状:口腔不卫生\n",
      "应对症状:保持口腔清洁\n",
      "针对症状:刷牙\n",
      "解决方案:\n",
      "1.每天早晚各刷一次牙\n",
      "2.使用含氟牙膏\n",
      "3.定期更换牙刷\n",
      "4.饭后用清水漱口\n",
      "5.避免吃过于辛辣刺激的食物\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.generation_config.top_p=0 # 只选择概率最高的token\n",
    "\n",
    "Q_list=['不小心怀孕了要怎么避孕？','我早上起来有过敏性鼻炎怎么办','最近嘴唇和手脚一直干裂','最近口腔感觉不卫生']\n",
    "for Q in Q_list:\n",
    "    prompt=prompt_template%(Q,)\n",
    "    print(prompt)\n",
    "    A,hist=model.chat(tokenizer,prompt,history=None)\n",
    "    print('Q:%s\\nA:%s\\n'%(Q,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f8922-df05-4c53-be64-05dfbf4950bf",
   "metadata": {},
   "source": [
    "# 单问题(无提示词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bfae9f-41f7-4110-a67e-5954e667dd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:最近嘴唇和手脚一直干裂？\n",
      "A:嘴唇和手脚干燥裂口可能是由于多种原因引起的，包括缺水、缺乏营养、皮肤疾病等。以下是一些可能的解决方案：\n",
      "\n",
      "1. 补水：每天喝足够的水可以帮助保持身体水分平衡，缓解干燥症状。\n",
      "\n",
      "2. 保湿霜：选择含有天然油脂或滋润成分的保湿霜，如乳液、润唇膏等，涂抹在嘴唇和手脚上，可以提供额外的保护和滋润。\n",
      "\n",
      "3. 饮食调整：多吃富含维生素A、C和E的食物，如胡萝卜、柑橘类水果、坚果等，这些食物有助于维护皮肤健康。\n",
      "\n",
      "4. 其他护理产品：使用含有甘油、尿囊素等保湿成分的产品，如护手霜、润肤露等，也可以帮助改善干燥状况。\n",
      "\n",
      "5. 如果以上方法都不能解决问题，建议就医检查，以排除其他可能的病因。\n",
      "\n",
      "请注意，每个人的体质和环境都不同，所以可能需要尝试不同的方法才能找到最适合自己的解决方案。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt='最近嘴唇和手脚一直干裂？'\n",
    "resp,hist=model.chat(tokenizer,prompt,history=None)\n",
    "print('Q:%s\\nA:%s\\n'%(prompt,resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa595c-ac14-4189-b6ae-995aa565f9c2",
   "metadata": {},
   "source": [
    "# 单问题(有提示词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1979717a-e5f6-4ca1-8917-fcab859182ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "给定一句话：“最近嘴唇和手脚一直干裂？”，请你按步骤要求工作。\n",
      "\n",
      "步骤1：识别用户问题中遇到身体上的常见症状，如感冒、发烧等。\n",
      "步骤2：根据问题，生成JSON格式字符串，格式为{\"①\":应对症状的一种做法, \"②\":针对症状的另一种做法}，根据具体症状给出一种或多种解决方案，分点作答\n",
      "\n",
      "请问，这个JSON字符串是：\n",
      "\n",
      "\n",
      "A:症状:嘴唇和手脚干裂\n",
      "应对症状:保持充足的水分摄入\n",
      "针对症状:使用润唇膏或者保湿霜\n",
      "解决方案:\n",
      "1.多喝水，保持身体水分充足\n",
      "2.避免过度口渴，可以适当喝一些含糖饮料\n",
      "3.涂抹润唇膏或者保湿霜，保持嘴唇湿润\n",
      "4.如果症状持续不减或者加重，建议就医\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q='最近嘴唇和手脚一直干裂？'\n",
    "prompt=prompt_template%(Q,)\n",
    "resp,hist=model.chat(tokenizer,prompt,history=None)\n",
    "print('Q:%s\\nA:%s\\n'%(prompt,resp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
